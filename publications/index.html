<!DOCTYPE html>
<html lang="en">
  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <!-- Metadata, OpenGraph and Schema.org -->




<!-- Standard metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<title>
  
  
    
      publications | Yekun Chai
    
  
</title>
<meta name="author" content="Yekun Chai">
<meta name="description" content="* indicates equal contribution; ^ denotes the student/researcher/intern I mentored.">

  <meta name="keywords" content="AI Research, LLM, Yekun Chai">










<!-- Bootstrap & MDB -->
<link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

<!-- Bootstrap Table -->


<!-- Fonts & Icons -->
<link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5">
<link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap">

<!-- Code Syntax Highlighting -->
<link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light">



<!-- Styles -->

<!-- pseudocode -->



  <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8C%80&lt;/text&gt;&lt;/svg&gt;">

<link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
<link rel="canonical" href="https://cyk1337.github.io/publications/">

<!-- Dark Mode -->
<script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script>

  <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark">
  <script>
    initTheme();
  </script>


<!-- GeoJSON support via Leaflet -->


<!-- diff2html -->






  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">
    <!-- Header -->
    <header>
  <!-- Nav Bar -->
  <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation">
    <div class="container">
      
        <a class="navbar-brand title font-weight-lighter" href="/">
          
            
              <span class="font-weight-bold">Yekun</span>
            
            
            Chai
          
        </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>

      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          

          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">about
              
            </a>
          </li>

          <!-- Other pages -->
          
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
              
                
                <li class="nav-item active">
                  
                  <a class="nav-link" href="/publications/">publications
                    
                      <span class="sr-only">(current)</span>
                    
                  </a>
                </li>
              
            
          
            
              
                
                <li class="nav-item ">
                  
                  <a class="nav-link" href="/notes/">notes
                    
                  </a>
                </li>
              
            
          
          
            <!-- Search -->
            <li class="nav-item">
              <button id="search-toggle" title="Search" onclick="openSearchModal()">
                <span class="nav-link">ctrl k <i class="ti ti-search"></i></span>
              </button>
            </li>
          
          
            <!-- Toogle theme mode -->
            <li class="toggle-container">
              <button id="light-toggle" title="Change theme">
                <i class="ti ti-sun-moon" id="light-toggle-system"></i>
                <i class="ti ti-moon-filled" id="light-toggle-dark"></i>
                <i class="ti ti-sun-filled" id="light-toggle-light"></i>
              </button>
            </li>
          
        </ul>
      </div>
    </div>
  </nav>
  
    <!-- Scrolling Progress Bar -->
    <progress id="progress" value="0">
      <div class="progress-container">
        <span class="progress-bar"></span>
      </div>
    </progress>
  
</header>


    <!-- Content -->
    <div class="container mt-5" role="main">
      
        <div class="post">
  <header class="post-header">
    <h1 class="post-title">publications</h1>
    <p class="post-description">* indicates equal contribution; ^ denotes the student/researcher/intern I mentored.</p>
  </header>

  <article>
    <!-- _pages/publications.md -->

<p><input type="text" id="bibsearch" spellcheck="false" autocomplete="off" class="search bibsearch-form-input" placeholder="Type to filter"></p>

<div class="publications">

<h2 class="bibliography">2025</h2>
<ol class="bibliography">
<li>
<div class="row">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded">EMNLP-Findings</abbr>
      
      
    </div>
  

  <!-- Entry bib key -->
  <div id="yu2025evolkvevolutionarykvcache" class="col-sm-8">
    <!-- Title -->
    <div class="title">EvolKV: Evolutionary KV Cache Compression for LLM Inference</div>
    <!-- Author -->
    <div class="author">
      

      
      Bohan
            Yu<sup>^</sup>, and <em>Yekun
            Chai<sup>†</sup></em>
      
        <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="† Corresponding author">
        </i>
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>EMNLP Findings</em>,  2025
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <!--
      
      -->
      
      
      
      
      
      
      
      
      
      
      
      
      
    </div>
    
      
      

      
      

      
      
      
    

    

    

    

    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded">EMNLP</abbr>
      
      
    </div>
  

  <!-- Entry bib key -->
  <div id="yang2025codemixbenchevaluatingcodemixingcapabilities" class="col-sm-8">
    <!-- Title -->
    <div class="title">CodeMixBench: Evaluating Code-Mixing Capabilities of LLMs Across 18 Languages</div>
    <!-- Author -->
    <div class="author">
      

      
      Yilun
            Yang<sup>^</sup>, and <em>Yekun
            Chai<sup>†</sup></em>
      
        <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="† Corresponding author">
        </i>
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>EMNLP</em>,  2025
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <!--
      
      -->
      
      
      
      
      
      
      
      
      
      
      
      
      
    </div>
    
      
      

      
      

      
      
      
    

    

    

    

    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded">EMNLP</abbr>
      
      
    </div>
  

  <!-- Entry bib key -->
  <div id="peng2025understanding" class="col-sm-8">
    <!-- Title -->
    <div class="title">Understanding Subword Compositionality of Large Language Models</div>
    <!-- Author -->
    <div class="author">
      

      
      <a href="https://punchwes.github.io/" rel="external nofollow noopener" target="_blank">Qiwei
              Peng</a>, <em>Yekun
            Chai</em>, and <a href="https://anderssoegaard.github.io/" rel="external nofollow noopener" target="_blank">Anders
              Søgaard</a>
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>EMNLP</em>,  2025
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <!--
      
      -->
      
      
      
      
      
      
      
      
      
      
      
      
      
    </div>
    
      
      

      
      

      
      
      
    

    

    

    

    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded">EMNLP</abbr>
      
      
    </div>
  

  <!-- Entry bib key -->
  <div id="peng2025debiasing" class="col-sm-8">
    <!-- Title -->
    <div class="title">Debiasing Multilingual LLMs in Cross-lingual Latent Space</div>
    <!-- Author -->
    <div class="author">
      

      
      <a href="https://punchwes.github.io/" rel="external nofollow noopener" target="_blank">Qiwei
              Peng</a>, Guimin
            Hu, <em>Yekun
            Chai</em>, and <a href="https://anderssoegaard.github.io/" rel="external nofollow noopener" target="_blank">Anders
              Søgaard</a>
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>EMNLP</em>,  2025
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <!--
      
      -->
      
      
      
      
      
      
      
      
      
      
      
      
      
    </div>
    
      
      

      
      

      
      
      
    

    

    

    

    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded">
            
              <a href="https://aclanthology.org/" rel="external nofollow noopener" target="_blank">ACL</a>
            
          </abbr>
      
      
    </div>
  

  <!-- Entry bib key -->
  <div id="sun-etal-2025-curiosity" class="col-sm-8">
    <!-- Title -->
    <div class="title">Curiosity-Driven Reinforcement Learning from Human Feedback</div>
    <!-- Author -->
    <div class="author">
      

      
      <a href="https://scholar.google.com/citations?user=zmLU3x0AAAAJ" rel="external nofollow noopener" target="_blank">Haoran
              Sun<sup>*^</sup></a>, <em>Yekun
            Chai<sup>*†</sup></em>, <a href="https://scholar.google.com/citations?user=fUkS6pAAAAAJ" rel="external nofollow noopener" target="_blank">Shuohuan
              Wang</a>
          , Yu
            Sun, and
        <span class="more-authors" title="click to view 2 more authors" onclick="
              var element = $(this);
              element.attr('title', '');
              var more_authors_text = element.text() == '2 more authors' ? 'Hua Wu, Haifeng Wang' : '2 more authors';
              var cursorPosition = 0;
              var textAdder = setInterval(function(){
                element.html(more_authors_text.substring(0, cursorPosition + 1));
                if (++cursorPosition == more_authors_text.length){
                  clearInterval(textAdder);
                }
            }, '10');
          ">2 more authors</span>
      
      
        <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="† Project lead">
        </i>
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>,  Jul 2025
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <!--
      
      -->
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      
      
      
        
          <a href="https://aclanthology.org/2025.acl-long.1146/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
        
      
      
        
          <a href="https://aclanthology.org/2025.acl-long.1146.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
        <a href="https://github.com/ernie-research/CD-RLHF" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      
      
      
      
    </div>
    
      
      

      
      

      
      
      
    

    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>Reinforcement learning from human feedback (RLHF) has proven effective in aligning large language models (LLMs) with human preferences, but often at the cost of reduced output diversity. This trade-off between diversity and alignment quality remains a significant challenge. Drawing inspiration from curiosity-driven exploration in reinforcement learning, we introduce curiosity-driven RLHF (CD-RLHF), a framework that incorporates intrinsic rewards for novel states, alongside traditional sparse extrinsic rewards, to optimize both output diversity and alignment quality. We demonstrate the effectiveness of CD-RLHF through extensive experiments on a range of tasks, including text summarization and instruction following. Our approach achieves significant gains in diversity on multiple diversity-oriented metrics while maintaining alignment with human preferences comparable to standard RLHF. We will make our code publicly available.</p>
      </div>
    

    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded">ICLR</abbr>
      
      
    </div>
  

  <!-- Entry bib key -->
  <div id="chai2025marlhf" class="col-sm-8">
    <!-- Title -->
    <div class="title">MA-RLHF: Reinforcement Learning from Human Feedback with Macro Actions</div>
    <!-- Author -->
    <div class="author">
      

      
      <em>Yekun
            Chai<sup>*†</sup></em>, <a href="https://scholar.google.com/citations?user=zmLU3x0AAAAJ" rel="external nofollow noopener" target="_blank">Haoran
              Sun<sup>*^</sup></a>, <a href="https://www.cs.ubc.ca/~hgfang/" rel="external nofollow noopener" target="_blank">Huang
              Fang</a>, <a href="https://scholar.google.com/citations?user=fUkS6pAAAAAJ" rel="external nofollow noopener" target="_blank">Shuohuan
              Wang</a>, and
        <span class="more-authors" title="click to view 2 more authors" onclick="
              var element = $(this);
              element.attr('title', '');
              var more_authors_text = element.text() == '2 more authors' ? 'Yu Sun, Hua Wu' : '2 more authors';
              var cursorPosition = 0;
              var textAdder = setInterval(function(){
                element.html(more_authors_text.substring(0, cursorPosition + 1));
                if (++cursorPosition == more_authors_text.length){
                  clearInterval(textAdder);
                }
            }, '10');
          ">2 more authors</span>
      
      
        <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="† Project lead">
        </i>
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In The Thirteenth International Conference on Learning Representations</em>,  Jul 2025
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <!--
      
      -->
      
      
      
      
      
        
          <a href="https://openreview.net/forum?id=WWXjMYZxfH" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
        
      
      
        
          <a href="https://openreview.net/pdf?id=WWXjMYZxfH" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
        <a href="https://github.com/ernie-research/MA-RLHF" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      
      
      
      
    </div>
    
      
      

      
      

      
      
      
    

    

    

    

    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded">COLING-Industry</abbr>
      
      
    </div>
  

  <!-- Entry bib key -->
  <div id="nakamura-etal-2025-aurora" class="col-sm-8">
    <!-- Title -->
    <div class="title">Aurora-M: Open Source Continual Pre-training for Multilingual Language and Code</div>
    <!-- Author -->
    <div class="author">
      

      
      Taishi
            Nakamura, Mayank
            Mishra, Simone
            Tedeschi, <em>Yekun
            Chai</em>, and
        <span class="more-authors" title="click to view 37 more authors" onclick="
              var element = $(this);
              element.attr('title', '');
              var more_authors_text = element.text() == '37 more authors' ? 'Jason T. Stillerman, Felix Friedrich, Prateek Yadav, Tanmay Laud, Vu Minh Chien, Terry Yue Zhuo, Diganta Misra, Ben Bogin, Xuan-Son Vu, Marzena Karpinska, Arnav Varma Dantuluri, Wojciech Kusa, Tommaso Furlanello, Rio Yokota, Niklas Muennighoff, Suhas Pai, Tosin Adewumi, Veronika Laippala, Xiaozhe Yao, Adalberto Barbosa Junior, Aleksandr Drozd, Jordan Clive, Kshitij Gupta, Liangyu Chen, Qi Sun, Ken Tsui, Nour Moustafa-Fahmy, Nicolo Monti, Tai Dang, Ziyang Luo, Tien-Tung Bui, Roberto Navigli, Virendra Mehta, Matthew Blumberg, Victor May, Hiep Nguyen, Sampo Pyysalo' : '37 more authors';
              var cursorPosition = 0;
              var textAdder = setInterval(function(){
                element.html(more_authors_text.substring(0, cursorPosition + 1));
                if (++cursorPosition == more_authors_text.length){
                  clearInterval(textAdder);
                }
            }, '10');
          ">37 more authors</span>
      
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In Proceedings of the 31st International Conference on Computational Linguistics: Industry Track</em>,  Jan 2025
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <!--
      
      -->
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      
      
      
        
          <a href="https://aclanthology.org/2025.coling-industry.56/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
        
      
      
        
          <a href="https://aclanthology.org/2025.coling-industry.56.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
        <a href="https://huggingface.co/blog/mayank-mishra/aurora" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Blog</a>
      
      
        <a href="https://huggingface.co/collections/aurora-m/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      
      
      
      
    </div>
    
      
      

      
      

      
      
      
    

    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>Pretrained language models are integral part of AI applications, but their high computational cost for training limits accessibility. Initiatives such as Bloom and StarCoder aim to democratize access to pretrained models for collaborative community development. Despite these efforts, such models encounter challenges such as limited multilingual capabilities, risks of catastrophic forgetting during continual pretraining, and the high costs of training models from scratch, alongside the need to align with AI safety standards and regulatory frameworks. This paper presents Aurora-M, a 15B parameter multilingual open-source model trained on English, Finnish, Hindi, Japanese, Vietnamese, and code. Continually pretrained from StarCoderPlus on 435B additional tokens, Aurora-M surpasses 2T tokens in total training token count. It is the first open-source multilingual model fine-tuned on human-reviewed safety instructions, thus aligning its development not only with conventional red-teaming considerations, but also with the specific concerns articulated in the Biden-Harris Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence. We evaluate Aurora-M across a wide range of tasks and languages, showcasing its robustness against catastrophic forgetting and its superior performance in multilingual settings, particularly in safety evaluations. We open-source Aurora-M and its variants to encourage responsible open-source development of large language models at https://huggingface.co/aurora-m.</p>
      </div>
    

    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded">COLING-Industry</abbr>
      
      
    </div>
  

  <!-- Entry bib key -->
  <div id="shen-chai-2025-graph" class="col-sm-8">
    <!-- Title -->
    <div class="title">Graph-Augmented Open-Domain Multi-Document Summarization</div>
    <!-- Author -->
    <div class="author">
      

      
      Xiaoping
            Shen<sup>†</sup>, and <em>Yekun
            Chai</em>
      
        <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="† Corresponding author">
        </i>
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In Proceedings of the 31st International Conference on Computational Linguistics: Industry Track</em>,  Jan 2025
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <!--
      
      -->
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      
      
      
        
          <a href="https://aclanthology.org/2025.coling-industry.27/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
        
      
      
        
          <a href="https://aclanthology.org/2025.coling-industry.27.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
      
      
      
    </div>
    
      
      

      
      

      
      
      
    

    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>In the open-domain multi-document summarization (ODMDS) task, retrieving relevant documents from large repositories and generating coherent summaries are crucial. However, existing methods often treat retrieval and summarization as separate tasks, neglecting the relationships among documents. To address these limitations, we propose an integrated retrieval-summarization framework that captures global document relationships through graph-based clustering, guiding the re-ranking of retrieved documents. This cluster-level thematic information is then used to guide large language models (LLMs) in refining the retrieved documents and generating more accurate, coherent summaries. Experimental results on the ODSUM benchmark demonstrate that our method significantly improves retrieval accuracy and produces summaries that surpass those derived from the oracle documents. These findings highlight the potential of our framework to improve both retrieval and summarization tasks in ODMDS.</p>
      </div>
    

    

    
  </div>
</div>
</li>
</ol>
<h2 class="bibliography">2024</h2>
<ol class="bibliography">
<li>
<div class="row">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded">Technical Report</abbr>
      
      
    </div>
  

  <!-- Entry bib key -->
  <div id="lozhkov2024starcoder" class="col-sm-8">
    <!-- Title -->
    <div class="title">StarCoder 2 and The Stack v2: The Next Generation</div>
    <!-- Author -->
    <div class="author">
      

      
      Anton
            Lozhkov
          , Raymond
            Li, Loubna Ben
            Allal, Federico
            Cassano, and
        <span class="more-authors" title="click to view 62 more authors" onclick="
              var element = $(this);
              element.attr('title', '');
              var more_authors_text = element.text() == '62 more authors' ? 'Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu, Max Tian, Denis Kocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu, Dmitry Abulkhanov, Indraneil Paul, Zhuang Li, Wen-Ding Li, Megan Risdal, Jia Li, Jian Zhu, Terry Yue Zhuo, Evgenii Zheltonozhskii, Nii Osae Osae Dade, Wenhao Yu, Lucas Krauß, Naman Jain, Yixuan Su, Xuanli He, Manan Dey, Edoardo Abati, Yekun Chai, Niklas Muennighoff, Xiangru Tang, Muhtasham Oblokulov, Christopher Akiki, Marc Marone, Chenghao Mou, Mayank Mishra, Alex Gu, Binyuan Hui, Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas Patry, Canwen Xu, Julian McAuley, Han Hu, Torsten Scholak, Sebastien Paquet, Jennifer Robinson, Carolyn Jane Anderson, Nicolas Chapados, Mostofa Patwary, Nima Tajbakhsh, Yacine Jernite, Carlos Muñoz Ferrandis, Lingming Zhang, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro Werra, Harm Vries' : '62 more authors';
              var cursorPosition = 0;
              var textAdder = setInterval(function(){
                element.html(more_authors_text.substring(0, cursorPosition + 1));
                if (++cursorPosition == more_authors_text.length){
                  clearInterval(textAdder);
                }
            }, '10');
          ">62 more authors</span>
      
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      Jan 2024
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <!--
      
      -->
      
      
      
      
      
      
        
          <a href="https://arxiv.org/pdf/2402.19173" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
        <a href="https://huggingface.co/blog/starcoder2" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Blog</a>
      
      
        <a href="https://github.com/bigcode-project/starcoder2" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      
      
      
      
    </div>
    
      
      

      
      

      
      
      
    

    

    

    

    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded">EMNLP</abbr>
      
      
    </div>
  

  <!-- Entry bib key -->
  <div id="chai-etal-2024-autoregressive" class="col-sm-8">
    <!-- Title -->
    <div class="title">Autoregressive Pre-Training on Pixels and Texts</div>
    <!-- Author -->
    <div class="author">
      

      
      <em>Yekun
            Chai</em>, Qingyi
            Liu<sup>^</sup>, Jingwu
            Xiao<sup>^</sup>, <a href="https://scholar.google.com/citations?user=fUkS6pAAAAAJ" rel="external nofollow noopener" target="_blank">Shuohuan
              Wang</a>, and
        <span class="more-authors" title="click to view 2 more authors" onclick="
              var element = $(this);
              element.attr('title', '');
              var more_authors_text = element.text() == '2 more authors' ? 'Yu Sun, Hua Wu' : '2 more authors';
              var cursorPosition = 0;
              var textAdder = setInterval(function(){
                element.html(more_authors_text.substring(0, cursorPosition + 1));
                if (++cursorPosition == more_authors_text.length){
                  clearInterval(textAdder);
                }
            }, '10');
          ">2 more authors</span>
      
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing</em>,  Nov 2024
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <!--
      
      -->
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      
      
      
        
          <a href="https://aclanthology.org/2024.emnlp-main.182" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
        
      
      
        
          <a href="https://aclanthology.org/2024.emnlp-main.182.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
        <a href="https://github.com/ernie-research/pixelgpt" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      
      
        
          <a href="/assets/pdf/poster/pixelgpt_emnlp24_poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a>
        
      
      
        
          <a href="/assets/pdf/slides/pixelgpt_emnlp24_slides.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a>
        
      
      
    </div>
    
      
      

      
      

      
      
      
    

    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>The integration of visual and textual information represents a promising direction in the advancement of language models. In this paper, we explore the dual modality of language—both visual and textual—within an autoregressive framework, pre-trained on both document images and texts. Our method employs a multimodal training strategy, utilizing visual data through next patch prediction with a regression head and/or textual data through next token prediction with a classification head. We focus on understanding the interaction between these two modalities and their combined impact on model performance. Our extensive evaluation across a wide range of benchmarks shows that incorporating both visual and textual data significantly improves the performance of pixel-based language models. Remarkably, we find that a unidirectional pixel-based model trained solely on visual data can achieve comparable results to state-of-the-art bidirectional models on several language understanding tasks. This work uncovers the untapped potential of integrating visual and textual modalities for more effective language modeling. We release our code, data, and model checkpoints at https://github.com/ernie-research/pixelgpt.</p>
      </div>
    

    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded">EMNLP</abbr><span class="award badge">Oral</span>
      
      
    </div>
  

  <!-- Entry bib key -->
  <div id="chai-etal-2024-training" class="col-sm-8">
    <!-- Title -->
    <div class="title">On Training Data Influence of GPT Models</div>
    <!-- Author -->
    <div class="author">
      

      
      <em>Yekun
            Chai</em>, Qingyi
            Liu<sup>^</sup>, <a href="https://scholar.google.com/citations?user=fUkS6pAAAAAJ" rel="external nofollow noopener" target="_blank">Shuohuan
              Wang</a>
          , Yu
            Sun, and
        <span class="more-authors" title="click to view 2 more authors" onclick="
              var element = $(this);
              element.attr('title', '');
              var more_authors_text = element.text() == '2 more authors' ? 'Qiwei Peng, Hua Wu' : '2 more authors';
              var cursorPosition = 0;
              var textAdder = setInterval(function(){
                element.html(more_authors_text.substring(0, cursorPosition + 1));
                if (++cursorPosition == more_authors_text.length){
                  clearInterval(textAdder);
                }
            }, '10');
          ">2 more authors</span>
      
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing</em>,  Nov 2024
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <!--
      
        <a class="award btn btn-sm z-depth-0" role="button">Awareded</a>
      
      -->
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      
      
      
        
          <a href="https://aclanthology.org/2024.emnlp-main.183" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
        
      
      
        
          <a href="https://aclanthology.org/2024.emnlp-main.183.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
        <a href="https://github.com/ernie-research/gptfluence" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      
      
        
          <a href="/assets/pdf/poster/gptfluence_emnlp24_poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a>
        
      
      
        
          <a href="/assets/pdf/slides/gptfluence_emnlp24_slides.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a>
        
      
      
    </div>
    
      
      

      
      

      
      
      
    

    
      <!-- Hidden Award block -->
      <div class="award hidden d-print-inline">
        <p></p>
<p>Oral</p>

      </div>
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>Amidst the rapid advancements in generative language models, the investigation of how training data shapes the performance of GPT models is still emerging. This paper presents GPTfluence, a novel approach that leverages a featurized simulation to assess the impact of training examples on the training dynamics of GPT models. Our approach not only traces the influence of individual training instances on performance trajectories, such as loss and other key metrics, on targeted test points but also enables a comprehensive comparison with existing methods across various training scenarios in GPT models, ranging from 14 million to 2.8 billion parameters, across a range of downstream tasks. Contrary to earlier methods that struggle with generalization to new data, GPTfluence introduces a parameterized simulation of training dynamics, demonstrating robust generalization capabilities to unseen training data. This adaptability is evident across both fine-tuning and instruction-tuning scenarios, spanning tasks in natural language understanding and generation. We make our code and data publicly available at https://github.com/ernie-research/gptfluence.</p>
      </div>
    

    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded">EMNLP-Findings</abbr><span class="award badge"></span>
      
      
    </div>
  

  <!-- Entry bib key -->
  <div id="chai-etal-2024-tokenization" class="col-sm-8">
    <!-- Title -->
    <div class="title">Tokenization Falling Short: On Subword Robustness in Large Language Models</div>
    <!-- Author -->
    <div class="author">
      

      
      <em>Yekun
            Chai</em>
          , Yewei
            Fang, <a href="https://punchwes.github.io/" rel="external nofollow noopener" target="_blank">Qiwei
              Peng</a>, and <a href="https://sites.google.com/view/lixuhong" rel="external nofollow noopener" target="_blank">Xuhong
              Li</a>
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In Findings of the Association for Computational Linguistics: EMNLP 2024</em>,  Nov 2024
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <!--
      
        <a class="award btn btn-sm z-depth-0" role="button">Awareded</a>
      
      -->
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      
      
      
        
          <a href="https://aclanthology.org/2024.findings-emnlp.86" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
        
      
      
        
          <a href="https://aclanthology.org/2024.findings-emnlp.86.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
        <a href="https://github.com/FloatAI/TKEval" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      
      
        
          <a href="/assets/pdf/poster/tkeval_emnlp24_poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a>
        
      
      
        
          <a href="/assets/pdf/slides/tkeval_emnlp24_slides.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a>
        
      
      
    </div>
    
      
      

      
      

      
      
      
    

    
      <!-- Hidden Award block -->
      <div class="award hidden d-print-inline">
        <p>
</p>
      </div>
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>Language models typically tokenize raw text into sequences of subword identifiers from a predefined vocabulary, a process inherently sensitive to typographical errors, length variations, and largely oblivious to the internal structure of tokens—issues we term *the curse of tokenization*. In this study, we delve into these drawbacks and demonstrate that large language models (LLMs) remain susceptible to these problems. This study systematically investigates these challenges and their impact on LLMs through three critical research questions: (1) complex problem solving, (2) token structure probing, and (3) resilience to typographical variation. Our findings reveal that scaling model parameters can mitigate the issue of tokenization; however, LLMs still suffer from biases induced by typos and other text format variations. Our experiments show that subword regularization such as BPE-dropout can mitigate this issue. We release our evaluation code and data at https://github.com/FloatAI/TKEval.</p>
      </div>
    

    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded">ICML</abbr>
      
      
    </div>
  

  <!-- Entry bib key -->
  <div id="li2024gilot" class="col-sm-8">
    <!-- Title -->
    <div class="title">GiLOT: Interpreting Generative Language Models via Optimal Transport</div>
    <!-- Author -->
    <div class="author">
      

      
      <a href="https://sites.google.com/view/lixuhong" rel="external nofollow noopener" target="_blank">Xuhong
              Li<sup>*</sup></a>, Jiamin
            Chen<sup>*</sup>, <em>Yekun
            Chai<sup>*</sup></em>, and <a href="https://sites.google.com/site/haoyixiongshomepage/" rel="external nofollow noopener" target="_blank">Haoyi
              Xiong</a>
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In Forty-first International Conference on Machine Learning</em>,  Nov 2024
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <!--
      
      -->
      
      
      
      
      
        
          <a href="https://icml.cc/virtual/2024/poster/32996" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
        
      
      
        
          <a href="https://openreview.net/pdf?id=qKL25sGjxL" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
        <a href="https://github.com/holyseven/GiLOT" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      
      
      
      
    </div>
    
      
      

      
      

      
      
      
    

    

    

    

    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded">LREC-COLING</abbr>
      
      
    </div>
  

  <!-- Entry bib key -->
  <div id="peng-etal-2024-humaneval-xl" class="col-sm-8">
    <!-- Title -->
    <div class="title">HumanEval-XL: A Multilingual Code Generation Benchmark for Cross-lingual Natural Language Generalization</div>
    <!-- Author -->
    <div class="author">
      

      
      <a href="https://punchwes.github.io/" rel="external nofollow noopener" target="_blank">Qiwei
              Peng<sup>*</sup></a>, <em>Yekun
            Chai<sup>*</sup></em>, and <a href="https://sites.google.com/view/lixuhong" rel="external nofollow noopener" target="_blank">Xuhong
              Li</a>
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)</em>,  May 2024
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <!--
      
      -->
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      
      
      
      
        
          <a href="https://aclanthology.org/2024.lrec-main.735.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
        <a href="https://github.com/FloatAI/humaneval-xl" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      
      
        
          <a href="/assets/pdf/poster/HumanEval-XL_COLING2024_poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a>
        
      
      
        
          <a href="/assets/pdf/slides/HumanEval-XL-COLING2024.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a>
        
      
      
    </div>
    
      
      

      
      

      
      
      
    

    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>Large language models (LLMs) have made significant progress in generating codes from textual prompts. However, existing benchmarks have mainly concentrated on translating English prompts to multilingual codes or have been constrained to very limited natural languages (NLs). These benchmarks have overlooked the vast landscape of massively multilingual NL to multilingual code, leaving a critical gap in the evaluation of multilingual LLMs. In response, we introduce HumanEval-XL, a massively multilingual code generation benchmark specifically crafted to address this deficiency. HumanEval-XL establishes connections between 23 NLs and 12 programming languages (PLs), and comprises of a collection of 22,080 prompts with an average of 8.33 test cases. By ensuring parallel data across multiple NLs and PLs, HumanEval-XL offers a comprehensive evaluation platform for multilingual LLMs, allowing the assessment of the understanding of different NLs. Our work serves as a pioneering step towards filling the void in evaluating NL generalization in the area of multilingual code generation. We make our evaluation code and data publicly available at https://github.com/FloatAI/HumanEval-XL.</p>
      </div>
    

    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded">ICLR</abbr><span class="award badge">Spotlight</span>
      
      
    </div>
  

  <!-- Entry bib key -->
  <div id="li2024toolaugmented" class="col-sm-8">
    <!-- Title -->
    <div class="title">Tool-Augmented Reward Modeling</div>
    <!-- Author -->
    <div class="author">
      

      
      
          Lei
            Li<sup>*^</sup>, <em>Yekun
            Chai<sup>*†</sup></em>, <a href="https://scholar.google.com/citations?user=fUkS6pAAAAAJ" rel="external nofollow noopener" target="_blank">Shuohuan
              Wang</a>
          , Yu
            Sun, and
        <span class="more-authors" title="click to view 3 more authors" onclick="
              var element = $(this);
              element.attr('title', '');
              var more_authors_text = element.text() == '3 more authors' ? 'Hao Tian, Ningyu Zhang, Hua Wu' : '3 more authors';
              var cursorPosition = 0;
              var textAdder = setInterval(function(){
                element.html(more_authors_text.substring(0, cursorPosition + 1));
                if (++cursorPosition == more_authors_text.length){
                  clearInterval(textAdder);
                }
            }, '10');
          ">3 more authors</span>
      
      
        <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="* Equal contribution&lt;br&gt;† Project lead">
        </i>
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In The Twelfth International Conference on Learning Representations</em> <u><small><i>(top 5%)</i></small></u>
,  May 2024
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <!--
      
        <a class="award btn btn-sm z-depth-0" role="button">Awareded</a>
      
      -->
      
      
      
      
      
        
          <a href="https://openreview.net/forum?id=d94x0gWTUX" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
        
      
      
        
          <a href="https://openreview.net/pdf?id=d94x0gWTUX" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
        <a href="https://github.com/ernie-research/Tool-Augmented-Reward-Model" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      
      
        
          <a href="/assets/pdf/poster/tarm_iclr24_poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a>
        
      
      
        
          <a href="https://iclr.cc/media/iclr-2024/Slides/18272.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Slides</a>
        
      
      
    </div>
    
      
      

      
      

      
      
      
    

    
      <!-- Hidden Award block -->
      <div class="award hidden d-print-inline">
        <p></p>
<p>Spotlight</p>

      </div>
    

    

    

    

    
  </div>
</div>
</li>
</ol>
<h2 class="bibliography">2023</h2>
<ol class="bibliography">
<li>
<div class="row">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded">ACL-Findings</abbr><span class="award badge"></span>
      
      
    </div>
  

  <!-- Entry bib key -->
  <div id="chai-etal-2023-ernie-code" class="col-sm-8">
    <!-- Title -->
    <div class="title">ERNIE-Code: Beyond English-Centric Cross-lingual Pretraining for Programming Languages</div>
    <!-- Author -->
    <div class="author">
      

      
      <em>Yekun
            Chai</em>, <a href="https://scholar.google.com/citations?user=fUkS6pAAAAAJ" rel="external nofollow noopener" target="_blank">Shuohuan
              Wang</a>, Chao
            Pang
          , Yu
            Sun, and
        <span class="more-authors" title="click to view 2 more authors" onclick="
              var element = $(this);
              element.attr('title', '');
              var more_authors_text = element.text() == '2 more authors' ? 'Hao Tian, Hua Wu' : '2 more authors';
              var cursorPosition = 0;
              var textAdder = setInterval(function(){
                element.html(more_authors_text.substring(0, cursorPosition + 1));
                if (++cursorPosition == more_authors_text.length){
                  clearInterval(textAdder);
                }
            }, '10');
          ">2 more authors</span>
      
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In Findings of the Association for Computational Linguistics: ACL 2023</em>
,  Jul 2023
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <!--
      
        <a class="award btn btn-sm z-depth-0" role="button">Awareded</a>
      
      -->
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      
      
      
        
          <a href="https://aclanthology.org/2023.findings-acl.676" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
        
      
      
        
          <a href="https://aclanthology.org/2023.findings-acl.676.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
        <a href="https://github.com/PaddlePaddle/PaddleNLP/blob/v2.8.1/model_zoo/ernie-code/README.en.md" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      
      
      
      
    </div>
    
      
      

      
      

      
      
      
    

    
      <!-- Hidden Award block -->
      <div class="award hidden d-print-inline">
        <p>
</p>
      </div>
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>Software engineers working with the same programming language (PL) may speak different natural languages (NLs) and vice versa, erecting huge barriers to communication and working efficiency. Recent studies have demonstrated the effectiveness of generative pre-training in computer programs, yet they are always English-centric. In this work, we step towards bridging the gap between multilingual NLs and multilingual PLs for large language models (LLMs). We release ERNIE-Code, a unified pre-trained language model for 116 NLs and 6 PLs. We employ two methods for universal cross-lingual pre-training: span-corruption language modeling that learns patterns from monolingual NL or PL; and pivot-based translation language modeling that relies on parallel data of many NLs and PLs. Extensive results show that ERNIE-Code outperforms previous multilingual LLMs for PL or NL across a wide range of end tasks of code intelligence, including multilingual code-to-text, text-to-code, code-to-code, and text-to-text generation. We further show its advantage of zero-shot prompting on multilingual code summarization and text-to-text translation. We release our code and pre-trained checkpoints.</p>
      </div>
    

    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded">NeurIPS</abbr><span class="award badge">Datasets and Benchmarks</span>
      
      
    </div>
  

  <!-- Entry bib key -->
  <div id="li-etal-2023-m4" class="col-sm-8">
    <!-- Title -->
    <div class="title">M<sup>4</sup>: A Unified XAI Benchmark for Faithfulness Evaluation of Feature Attribution Methods across Metrics, Modalities and Models</div>
    <!-- Author -->
    <div class="author">
      

      
      <a href="https://sites.google.com/view/lixuhong" rel="external nofollow noopener" target="_blank">Xuhong
              Li</a>, <a href="https://mengnandu.com/" rel="external nofollow noopener" target="_blank">Mengnan
              Du</a>, Jiamin
            Chen, <em>Yekun
            Chai</em>, and
        <span class="more-authors" title="click to view 2 more authors" onclick="
              var element = $(this);
              element.attr('title', '');
              var more_authors_text = element.text() == '2 more authors' ? 'Himabindu Lakkaraju, Haoyi Xiong' : '2 more authors';
              var cursorPosition = 0;
              var textAdder = setInterval(function(){
                element.html(more_authors_text.substring(0, cursorPosition + 1));
                if (++cursorPosition == more_authors_text.length){
                  clearInterval(textAdder);
                }
            }, '10');
          ">2 more authors</span>
      
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track</em>,  Jul 2023
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <!--
      
        <a class="award btn btn-sm z-depth-0" role="button">Awareded</a>
      
      -->
      
      
      
      
      
      
        
          <a href="https://openreview.net/pdf?id=6zcfrSz98y" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
        <a href="https://github.com/PaddlePaddle/InterpretDL" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      
      
        
          <a href="/assets/pdf/poster/M4.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a>
        
      
      
        
          <a href="/assets/pdf/slides/M4-NeurIPS2023.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a>
        
      
      
    </div>
    
      
      

      
      

      
      
      
    

    
      <!-- Hidden Award block -->
      <div class="award hidden d-print-inline">
        <p></p>
<p>Datasets and Benchmarks</p>

      </div>
    

    

    

    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded">IJCNLP-AACL</abbr><span class="award badge">Demos</span>
      
      
    </div>
  

  <!-- Entry bib key -->
  <div id="zhu-etal-2023-ernie" class="col-sm-8">
    <!-- Title -->
    <div class="title">ERNIE-Music: Text-to-Waveform Music Generation with Diffusion Models</div>
    <!-- Author -->
    <div class="author">
      

      
      Pengfei
            Zhu<sup>^</sup>, Chao
            Pang, <em>Yekun
            Chai</em>
          , Lei
            Li<sup>^</sup>, and
        <span class="more-authors" title="click to view 4 more authors" onclick="
              var element = $(this);
              element.attr('title', '');
              var more_authors_text = element.text() == '4 more authors' ? 'Shuohuan Wang, Yu Sun, Hao Tian, Hua Wu' : '4 more authors';
              var cursorPosition = 0;
              var textAdder = setInterval(function(){
                element.html(more_authors_text.substring(0, cursorPosition + 1));
                if (++cursorPosition == more_authors_text.length){
                  clearInterval(textAdder);
                }
            }, '10');
          ">4 more authors</span>
      
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics: System Demonstrations</em>,  Nov 2023
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <!--
      
        <a class="award btn btn-sm z-depth-0" role="button">Awareded</a>
      
      -->
      
      
      
      
      
      
        
          <a href="https://aclanthology.org/2023.ijcnlp-demo.9.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
      
      
      
    </div>
    
      
      

      
      

      
      
      
    

    
      <!-- Hidden Award block -->
      <div class="award hidden d-print-inline">
        <p></p>
<p>Demos</p>

      </div>
    

    

    

    

    
  </div>
</div>
</li>
</ol>
<h2 class="bibliography">2022</h2>
<ol class="bibliography">
<li>
<div class="row">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded">EMNLP-Findings</abbr><span class="award badge"></span>
      
      
    </div>
  

  <!-- Entry bib key -->
  <div id="chai-etal-2022-clip" class="col-sm-8">
    <!-- Title -->
    <div class="title">Clip-Tuning: Towards Derivative-free Prompt Learning with a Mixture of Rewards</div>
    <!-- Author -->
    <div class="author">
      

      
      <em>Yekun
            Chai</em>, <a href="https://scholar.google.com/citations?user=fUkS6pAAAAAJ" rel="external nofollow noopener" target="_blank">Shuohuan
              Wang</a>
          , Yu
            Sun, <a href="https://scholar.google.com/citations?user=GNfC6yYAAAAJ" rel="external nofollow noopener" target="_blank">Hao
              Tian</a>, and
        <span class="more-authors" title="click to view 2 more authors" onclick="
              var element = $(this);
              element.attr('title', '');
              var more_authors_text = element.text() == '2 more authors' ? 'Hua Wu, Haifeng Wang' : '2 more authors';
              var cursorPosition = 0;
              var textAdder = setInterval(function(){
                element.html(more_authors_text.substring(0, cursorPosition + 1));
                if (++cursorPosition == more_authors_text.length){
                  clearInterval(textAdder);
                }
            }, '10');
          ">2 more authors</span>
      
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In Findings of the Association for Computational Linguistics: EMNLP 2022</em>,  Dec 2022
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <!--
      
        <a class="award btn btn-sm z-depth-0" role="button">Awareded</a>
      
      -->
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      
      
      
      
        
          <a href="https://aclanthology.org/2022.findings-emnlp.8.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
      
      
      
    </div>
    
      
      

      
      

      
      
      
    

    
      <!-- Hidden Award block -->
      <div class="award hidden d-print-inline">
        <p>
</p>
      </div>
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>Derivative-free prompt learning has emerged as a lightweight alternative to prompt tuning, which only requires model inference to optimize the prompts. However, existing work did not take full advantage of the over-parameterized characteristics of large pre-trained language models (PLMs). In this paper, we propose Clip-Tuning, a simple yet effective method that adopts diverse frozen “thinned” networks of PLMs to obtain *a mixture of rewards* and thus advance the derivative-free prompt learning. The thinned networks consist of all the hidden units that survive a stationary dropout strategy, whose inference predictions reflect an ensemble of partial views over prompted training samples. Our method outperforms previous gradient-free prompt learning methods and achieves parity with gradient-based counterparts on seven language understanding benchmarks under few-shot settings.</p>
      </div>
    

    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded">
            
              <a href="https://aclanthology.org/" rel="external nofollow noopener" target="_blank">ACL</a>
            
          </abbr><span class="award badge">Oral</span>
      
      
    </div>
  

  <!-- Entry bib key -->
  <div id="peng-etal-2022-predicate" class="col-sm-8">
    <!-- Title -->
    <div class="title">Predicate-Argument Based Bi-Encoder for Paraphrase Identification</div>
    <!-- Author -->
    <div class="author">
      

      
      <a href="https://punchwes.github.io/" rel="external nofollow noopener" target="_blank">Qiwei
              Peng</a>, <a href="https://profiles.sussex.ac.uk/p2860-david-weir" rel="external nofollow noopener" target="_blank">David
              Weir</a>, Julie
            Weeds, and <em>Yekun
            Chai</em>
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>,  May 2022
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <!--
      
        <a class="award btn btn-sm z-depth-0" role="button">Awareded</a>
      
      -->
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      
      
      
      
        
          <a href="https://aclanthology.org/2022.acl-long.382.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
      
        
          <a href="/assets/pdf/poster/PI_PAS_ACL2022.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a>
        
      
      
        
          <a href="/assets/pdf/slides/PI-PAS-ACL2022.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a>
        
      
      
    </div>
    
      
      

      
      

      
      
      
    

    
      <!-- Hidden Award block -->
      <div class="award hidden d-print-inline">
        <p></p>
<p>Oral</p>

      </div>
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>Paraphrase identification involves identifying whether a pair of sentences express the same or similar meanings. While cross-encoders have achieved high performances across several benchmarks, bi-encoders such as SBERT have been widely applied to sentence pair tasks. They exhibit substantially lower computation complexity and are better suited to symmetric tasks. In this work, we adopt a bi-encoder approach to the paraphrase identification task, and investigate the impact of explicitly incorporating predicate-argument information into SBERT through weighted aggregation. Experiments on six paraphrase identification datasets demonstrate that, with a minimal increase in parameters, the proposed model is able to outperform SBERT/SRoBERTa significantly. Further, ablation studies reveal that the predicate-argument based component plays a significant role in the performance gain.</p>
      </div>
    

    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded">SemEval</abbr>
      
      
    </div>
  

  <!-- Entry bib key -->
  <div id="han-etal-2022-x" class="col-sm-8">
    <!-- Title -->
    <div class="title">X-PuDu at SemEval-2022 Task 6: Multilingual Learning for English and Arabic Sarcasm Detection</div>
    <!-- Author -->
    <div class="author">
      

      
      Yaqian
            Han, <em>Yekun
            Chai</em>, <a href="https://scholar.google.com/citations?user=fUkS6pAAAAAJ" rel="external nofollow noopener" target="_blank">Shuohuan
              Wang</a>
          , Yu
            Sun, and
        <span class="more-authors" title="click to view 4 more authors" onclick="
              var element = $(this);
              element.attr('title', '');
              var more_authors_text = element.text() == '4 more authors' ? 'Hongyi Huang, Guanghao Chen, Yitong Xu, Yang Yang' : '4 more authors';
              var cursorPosition = 0;
              var textAdder = setInterval(function(){
                element.html(more_authors_text.substring(0, cursorPosition + 1));
                if (++cursorPosition == more_authors_text.length){
                  clearInterval(textAdder);
                }
            }, '10');
          ">4 more authors</span>
      
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In Proceedings of the 16th International Workshop on Semantic Evaluation (SemEval-2022)</em>,  Jul 2022
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <!--
      
      -->
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      
      
      
      
        
          <a href="https://aclanthology.org/2022.semeval-1.140.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
      
        
          <a href="/assets/pdf/poster/SemEval2022_task6_poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a>
        
      
      
      
    </div>
    
      
      

      
      

      
      
      
    

    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>Detecting sarcasm and verbal irony from people’s subjective statements is crucial to understanding their intended meanings and real sentiments and positions in social scenarios. This paper describes the X-PuDu system that participated in SemEval-2022 Task 6, iSarcasmEval - Intended Sarcasm Detection in English and Arabic, which aims at detecting intended sarcasm in various settings of natural language understanding. Our solution finetunes pre-trained language models, such as ERNIE-M and DeBERTa, under the multilingual settings to recognize the irony from Arabic and English texts. Our system ranked second out of 43, and ninth out of 32 in Task A: one-sentence detection in English and Arabic; fifth out of 22 in Task B: binary multi-label classification in English; first out of 16, and fifth out of 13 in Task C: sentence-pair detection in English and Arabic.</p>
      </div>
    

    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded">ICASSP</abbr><span class="award badge">Oral</span>
      
      
    </div>
  

  <!-- Entry bib key -->
  <div id="chai2023improved" class="col-sm-8">
    <!-- Title -->
    <div class="title">Improved Training of Mixture-of-Experts Language GANs</div>
    <!-- Author -->
    <div class="author">
      

      
      <em>Yekun
            Chai</em>, <a href="https://scholar.google.com/citations?user=glvvuKUAAAAJ" rel="external nofollow noopener" target="_blank">Qiyue
              Yin</a>
          , and Junge
            Zhang
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>,  Jul 2022
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <!--
      
        <a class="award btn btn-sm z-depth-0" role="button">Awareded</a>
      
      -->
      
      
      
      
      
      
        
          <a href="https://arxiv.org/pdf/2302.11875.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
      
      
      
    </div>
    
      
      

      
      

      
      
      
    

    
      <!-- Hidden Award block -->
      <div class="award hidden d-print-inline">
        <p></p>
<p>Oral</p>

      </div>
    

    

    

    

    
  </div>
</div>
</li>
</ol>
<h2 class="bibliography">2021</h2>
<ol class="bibliography">
<li>
<div class="row">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded">EMNLP-Findings</abbr><span class="award badge"></span>
      
      
    </div>
  

  <!-- Entry bib key -->
  <div id="chai-etal-2021-counter-contrastive" class="col-sm-8">
    <!-- Title -->
    <div class="title">Counter-Contrastive Learning for Language GANs</div>
    <!-- Author -->
    <div class="author">
      

      
      <em>Yekun
            Chai</em>
          , Haidong
            Zhang, <a href="https://scholar.google.com/citations?user=glvvuKUAAAAJ" rel="external nofollow noopener" target="_blank">Qiyue
              Yin</a>
          , and Junge
            Zhang
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In Findings of the Association for Computational Linguistics: EMNLP 2021</em>,  Nov 2021
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <!--
      
        <a class="award btn btn-sm z-depth-0" role="button">Awareded</a>
      
      -->
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      
      
      
      
        
          <a href="https://aclanthology.org/2021.findings-emnlp.415.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
      
        
          <a href="/assets/pdf/poster/CCL_poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a>
        
      
      
        
          <a href="/assets/pdf/slides/CCL-EMNLP2021.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a>
        
      
      
    </div>
    
      
      

      
      

      
      
      
    

    
      <!-- Hidden Award block -->
      <div class="award hidden d-print-inline">
        <p>
</p>
      </div>
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>Generative Adversarial Networks (GANs) have achieved great success in image synthesis, but have proven to be difficult to generate natural language. Challenges arise from the uninformative learning signals passed from the discriminator. In other words, the poor learning signals limit the learning capacity for generating languages with rich structures and semantics. In this paper, we propose to adopt the counter-contrastive learning (CCL) method to support the generator’s training in language GANs. In contrast to standard GANs that adopt a simple binary classifier to discriminate whether a sample is real or fake, we employ a counter-contrastive learning signal that advances the training of language synthesizers by (1) pulling the language representations of generated and real samples together and (2) pushing apart representations of real samples to compete with the discriminator and thus prevent the discriminator from being overtrained. We evaluate our method on both synthetic and real benchmarks and yield competitive performance compared to previous GANs for adversarial sequence generation.</p>
      </div>
    

    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded">NAACL Workshop</abbr><span class="award badge"></span>
      
      
    </div>
  

  <!-- Entry bib key -->
  <div id="Chai2021RefineCapCR" class="col-sm-8">
    <!-- Title -->
    <div class="title">RefineCap: Concept-Aware Refinement for Image Captioning</div>
    <!-- Author -->
    <div class="author">
      

      
      <em>Yekun
            Chai</em>, Shuo
            Jin, and Junliang
            Xing
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>NAACL Workshop on Visually Grounded Interaction and Language (ViGiL)</em>,  Jun 2021
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <!--
      
        <a class="award btn btn-sm z-depth-0" role="button">Awareded</a>
      
      -->
      
      
        <a class="abstract btn btn-sm z-depth-0" role="button">TL;DR</a>
      
      
      
      
      
        
          <a href="https://vigilworkshop.github.io/static/papers-2021/4.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
      
      
      
    </div>
    
      
      

      
      

      
      
      
    

    
      <!-- Hidden Award block -->
      <div class="award hidden d-print-inline">
        <p>
</p>
      </div>
    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>RL fine-tuning on visual language models.</p>
      </div>
    

    

    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded">NAACL Workshop</abbr><span class="award badge"></span>
      
      
    </div>
  

  <!-- Entry bib key -->
  <div id="zhang-chai-2021-coin" class="col-sm-8">
    <!-- Title -->
    <div class="title">COIN: Conversational Interactive Networks for Emotion Recognition in Conversation</div>
    <!-- Author -->
    <div class="author">
      

      
      
          Haidong
            Zhang<sup>*</sup>, and <em>Yekun
            Chai<sup>*</sup></em>
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In Proceedings of the Third Workshop on Multimodal Artificial Intelligence</em>,  Jun 2021
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <!--
      
        <a class="award btn btn-sm z-depth-0" role="button">Awareded</a>
      
      -->
      
      
      
      
      
        
          <a href="https://aclanthology.org/2021.maiworkshop-1.3/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
        
      
      
        
          <a href="https://aclanthology.org/2021.maiworkshop-1.3.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
      
      
      
    </div>
    
      
      

      
      

      
      
      
    

    
      <!-- Hidden Award block -->
      <div class="award hidden d-print-inline">
        <p>
</p>
      </div>
    

    

    

    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded">IJCNN</abbr><span class="award badge">oral</span>
      
      
    </div>
  

  <!-- Entry bib key -->
  <div id="chai2023neural" class="col-sm-8">
    <!-- Title -->
    <div class="title">Neural Text Classification by Jointly Learning to Cluster and Align</div>
    <!-- Author -->
    <div class="author">
      

      
      <em>Yekun
            Chai</em>
          , Haidong
            Zhang, <a href="https://scholar.google.com/citations?user=glvvuKUAAAAJ" rel="external nofollow noopener" target="_blank">Qiyue
              Yin</a>
          , and Junge
            Zhang
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In International Joint Conference on Neural Networks</em>,   2021
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <!--
      
        <a class="award btn btn-sm z-depth-0" role="button">Awareded</a>
      
      -->
      
      
      
      
      
      
        
          <a href="https://arxiv.org/pdf/2011.12184" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
      
      
      
    </div>
    
      
      

      
      

      
      
      
    

    
      <!-- Hidden Award block -->
      <div class="award hidden d-print-inline">
        <p></p>
<p>oral</p>

      </div>
    

    

    

    

    
  </div>
</div>
</li>
</ol>
<h2 class="bibliography">2020</h2>
<ol class="bibliography"><li>
<div class="row">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded">
            
              <a href="https://aclanthology.org/" rel="external nofollow noopener" target="_blank">ACL</a>
            
          </abbr>
      
      
    </div>
  

  <!-- Entry bib key -->
  <div id="chai-etal-2020-highway" class="col-sm-8">
    <!-- Title -->
    <div class="title">Highway Transformer: Self-Gating Enhanced Self-Attentive Networks</div>
    <!-- Author -->
    <div class="author">
      

      
      <em>Yekun
            Chai</em>, Shuo
            Jin, and <a href="https://scholar.google.com.hk/citations?user=WFsqZskAAAAJ" rel="external nofollow noopener" target="_blank">Xinwen
              Hou</a>
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>,  Jul 2020
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <!--
      
      -->
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      
      
      
      
        
          <a href="https://aclanthology.org/2020.acl-main.616.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
        <a href="https://github.com/cyk1337/Highway-Transformer" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      
      
      
        
          <a href="/assets/pdf/slides/SDU-ACL2020.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a>
        
      
      
    </div>
    
      
      

      
      

      
      
      
    

    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>Self-attention mechanisms have made striking state-of-the-art (SOTA) progress in various sequence learning tasks, standing on the multi-headed dot product attention by attending to all the global contexts at different locations. Through a pseudo information highway, we introduce a gated component self-dependency units (SDU) that incorporates LSTM-styled gating units to replenish internal semantic importance within the multi-dimensional latent space of individual representations. The subsidiary content-based SDU gates allow for the information flow of modulated latent embeddings through skipped connections, leading to a clear margin of convergence speed with gradient descent algorithms. We may unveil the role of gating mechanism to aid in the context-based Transformer modules, with hypothesizing that SDU gates, especially on shallow layers, could push it faster to step towards suboptimal points during the optimization process.</p>
      </div>
    

    

    
  </div>
</div>
</li></ol>

</div>

  </article>

  

  
</div>

      
    </div>

    <!-- Footer -->
    
  <footer class="fixed-bottom" role="contentinfo">
    <div class="container mt-0">
      © Copyright 2025
      Yekun
      
      Chai. 
      
      <!-- cyk added START -->
      
        <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
        </script>
        <span id="busuanzi_value_site_uv"> </span> visitors.
      
      <!-- cyk added END -->

      
    </div>
  </footer>



    <!-- JavaScripts -->
    <!-- jQuery -->
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
<script src="/assets/js/bootstrap.bundle.min.js"></script>
<!-- <script src="/assets/js/mdb.min.js"></script> -->
<script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    
  <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>


    

    

    

    

    

    

    

    

    
  <!-- Enable Tooltips -->
  <script type="text/javascript">
    $(function () {
      $('[data-toggle="tooltip"]').tooltip();
    });
  </script>


  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script>



<!-- Bootstrap Table -->


<!-- Load Common JS -->
<script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script>
<script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script>
<script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script>

<!-- Bibsearch Feature -->

  <script src="/assets/js/bibsearch.js?a8796296a5e2f80c5e498cdd51d7761e" type="module"></script>


<!-- Jupyter Open External Links New Tab -->
<script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script>



    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>


  <script async src="https://badge.dimensions.ai/badge.js"></script>


    
  
    <!-- MathJax -->
    <script type="text/javascript">
      window.MathJax = {
        tex: {
          tags: 'ams',
        },
      };
    </script>
    <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script>
    <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script>
  


    

    



    
  <!-- Scrolling Progress Bar -->
  <script type="text/javascript">
    /*
     * This JavaScript code has been adapted from the article
     * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar,
     * published on the website https://css-tricks.com on the 7th of May, 2014.
     * Couple of changes were made to the original code to make it compatible
     * with the `al-foio` theme.
     */
    const progressBar = $('#progress');
    /*
     * We set up the bar after all elements are done loading.
     * In some cases, if the images in the page are larger than the intended
     * size they'll have on the page, they'll be resized via CSS to accomodate
     * the desired size. This mistake, however, breaks the computations as the
     * scroll size is computed as soon as the elements finish loading.
     * To account for this, a minimal delay was introduced before computing the
     * values.
     */
    window.onload = function () {
      setTimeout(progressBarSetup, 50);
    };
    /*
     * We set up the bar according to the browser.
     * If the browser supports the progress element we use that.
     * Otherwise, we resize the bar thru CSS styling
     */
    function progressBarSetup() {
      if ('max' in document.createElement('progress')) {
        initializeProgressElement();
        $(document).on('scroll', function () {
          progressBar.attr({ value: getCurrentScrollPosition() });
        });
        $(window).on('resize', initializeProgressElement);
      } else {
        resizeProgressBar();
        $(document).on('scroll', resizeProgressBar);
        $(window).on('resize', resizeProgressBar);
      }
    }
    /*
     * The vertical scroll position is the same as the number of pixels that
     * are hidden from view above the scrollable area. Thus, a value > 0 is
     * how much the user has scrolled from the top
     */
    function getCurrentScrollPosition() {
      return $(window).scrollTop();
    }

    function initializeProgressElement() {
      let navbarHeight = $('#navbar').outerHeight(true);
      $('body').css({ 'padding-top': navbarHeight });
      $('progress-container').css({ 'padding-top': navbarHeight });
      progressBar.css({ top: navbarHeight });
      progressBar.attr({
        max: getDistanceToScroll(),
        value: getCurrentScrollPosition(),
      });
    }
    /*
     * The offset between the html document height and the browser viewport
     * height will be greater than zero if vertical scroll is possible.
     * This is the distance the user can scroll
     */
    function getDistanceToScroll() {
      return $(document).height() - $(window).height();
    }

    function resizeProgressBar() {
      progressBar.css({ width: getWidthPercentage() + '%' });
    }
    // The scroll ratio equals the percentage to resize the bar
    function getWidthPercentage() {
      return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
    }
  </script>


    

    

    

    
  <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script>
  <script>
    addBackToTop();
  </script>


    
  <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script>
  <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys>
  <script>
    let searchTheme = determineComputedTheme();
    const ninjaKeys = document.querySelector('ninja-keys');

    if (searchTheme === 'dark') {
      ninjaKeys.classList.add('dark');
    } else {
      ninjaKeys.classList.remove('dark');
    }

    const openSearchModal = () => {
      // collapse navbarNav if expanded on mobile
      const $navbarNav = $('#navbarNav');
      if ($navbarNav.hasClass('show')) {
        $navbarNav.collapse('hide');
      }
      ninjaKeys.open();
    };
  </script>
  <script>
    // get the ninja-keys element
    const ninja = document.querySelector('ninja-keys');

    // add the home and posts menu items
    ninja.data = [{
        id: "nav-about",
        title: "about",
        section: "Navigation",
        handler: () => {
          window.location.href = "/";
        },
      },{id: "nav-publications",
              title: "publications",
              description: "* indicates equal contribution; ^ denotes the student/researcher/intern I mentored.",
              section: "Navigation",
              handler: () => {
                window.location.href = "/publications/";
              },
            },{id: "nav-notes",
              title: "notes",
              description: "",
              section: "Navigation",
              handler: () => {
                window.location.href = "/notes/";
              },
            },{id: "news-we-are-excited-to-announce-the-release-of-ernie-bot-文心一言-baidu-s-cutting-edge-chatgpt-like-ai-product-explore-it-at-https-yiyan-baidu-com-rocket",
              title: 'We are excited to announce the release of ERNIE-Bot(文心一言), Baidu’s cutting-edge ChatGPT-like AI...',
              description: "",
              section: "News",},{id: "news-ernie-code-on-multilingual-text-and-code-pre-training-has-been-accepted-to-acl-findings-2023-check-our-code-and-models-snowflake",
              title: 'ERNIE-Code on multilingual text and code pre-training has been accepted to ACL (Findings)...',
              description: "",
              section: "News",},{id: "news-one-paper-on-xai-has-been-accepted-to-neurips-2023-datasets-and-benchmarks-track-code-is-available-here",
              title: 'One paper on XAI has been accepted to NeurIPS 2023 datasets and benchmarks...',
              description: "",
              section: "News",},{id: "news-one-paper-on-tool-augmented-reward-models-has-been-accepted-to-iclr-2024-spotlight-sparkles-dive-into-our-research-and-code-now-fire",
              title: 'One paper on tool-augmented reward models has been accepted to ICLR 2024 (spotlight)<img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20">....',
              description: "",
              section: "News",},{id: "news-one-paper-on-humaneval-xl-a-multilingual-code-generation-benchmark-has-been-accepted-to-lrec-coling-2024-we-ve-released-the-code-and-data",
              title: 'One paper on HumanEval-XL, a multilingual code generation benchmark has been accepted to...',
              description: "",
              section: "News",},{id: "news-one-paper-on-gilot-an-xai-approach-for-llms-has-been-accepted-to-icml-2024-snowflake",
              title: 'One paper on GiLOT, an XAI approach for LLMs, has been accepted to...',
              description: "",
              section: "News",},{id: "news-our-papers-on-pixelgpt-gptfluence-and-tkeval-have-been-accepted-to-emnlp-2024-amp-amp-findings-bear",
              title: 'Our papers on PixelGPT, GPTfluence, and TKEval have been accepted to EMNLP 2024...',
              description: "",
              section: "News",},{id: "news-one-paper-on-macro-action-rlhf-has-been-accepted-to-iclr-2025-bear-dive-into-our-research-and-code-now-fire",
              title: 'One paper on macro action RLHF has been accepted to ICLR 2025<img class="emoji" title=":bear:" alt=":bear:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f43b.png" height="20" width="20">. Dive...',
              description: "",
              section: "News",},{id: "news-one-paper-on-curiosity-driven-rlhf-has-been-accepted-to-acl-2025",
              title: 'One paper on curiosity-driven RLHF has been accepted to ACL 2025.',
              description: "",
              section: "News",},{id: "news-four-papers-have-been-accepted-to-emnlp-2025-amp-amp-findings-bear",
              title: 'Four papers have been accepted to EMNLP 2025 &amp; Findings. <img class="emoji" title=":bear:" alt=":bear:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f43b.png" height="20" width="20">',
              description: "",
              section: "News",},{
          id: 'light-theme',
          title: 'Change theme to light',
          description: 'Change the theme of the site to Light',
          section: 'Theme',
          handler: () => {
            setThemeSetting("light");
          },
        },
        {
          id: 'dark-theme',
          title: 'Change theme to dark',
          description: 'Change the theme of the site to Dark',
          section: 'Theme',
          handler: () => {
            setThemeSetting("dark");
          },
        },
        {
          id: 'system-theme',
          title: 'Use system default theme',
          description: 'Change the theme of the site to System Default',
          section: 'Theme',
          handler: () => {
            setThemeSetting("system");
          },
        },];
  </script>
  <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script>


  </body>
</html>
